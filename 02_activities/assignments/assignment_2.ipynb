{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv('c:/Tina Lin/Training/DSI/Week10_Production/assignments/05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (22792, 14)\n",
      "Testing set shape: (9769, 14)\n",
      "Training target shape: (22792,)\n",
      "Testing target shape: (9769,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create DataFrame X with features (all columns except 'income')\n",
    "X = adult_dt.drop(columns='income')\n",
    "\n",
    "# Create DataFrame Y with target data (only the 'income' column)\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", Y_train.shape)\n",
    "print(\"Testing target shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "\n",
    "[Answer] The random state parameter in functions like train_test_split in scikit-learn is used to control the randomness of the data splitting process. You can set random_state to any integer value. Common practice is to use simple numbers like 0, 42, or any number you choose.\n",
    "\n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?\n",
    "\n",
    "[Answer] The random_state parameter is vital for ensuring reproducibility, facilitating reliable evaluations, and maintaining consistency in experiments. This makes it a fundamental practice in data science and machine learning workflows, enabling data scientists and researchers to produce reliable and valid results that can be trusted and replicated by others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Comment here.)*\n",
    "\n",
    "Question1: [Answer] The random state parameter in functions like train_test_split in scikit-learn is used to control the randomness of the data splitting process. You can set random_state to any integer value. Common practice is to use simple numbers like 0, 42, or any number you choose.\n",
    "\n",
    "Question2: [Answer] The random_state parameter is vital for ensuring reproducibility, facilitating reliable evaluations, and maintaining consistency in experiments. This makes it a fundamental practice in data science and machine learning workflows, enabling data scientists and researchers to produce reliable and valid results that can be trusted and replicated by others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  <Compressed Sparse Row sparse matrix of dtype ...\n",
      "1  <Compressed Sparse Row sparse matrix of dtype ...\n",
      "2  <Compressed Sparse Row sparse matrix of dtype ...\n",
      "3  <Compressed Sparse Row sparse matrix of dtype ...\n",
      "4  <Compressed Sparse Row sparse matrix of dtype ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = [\n",
    "    'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', \n",
    "         Pipeline(steps=[\n",
    "             ('knn_imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "             ('scaler', RobustScaler())\n",
    "         ]), \n",
    "         numerical_cols),\n",
    "        \n",
    "        ('cat', \n",
    "         Pipeline(steps=[\n",
    "             ('simple_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "         ]), \n",
    "         categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use this preprocessor to fit and transform your data\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "X_transformed_df = pd.DataFrame(X_transformed)\n",
    "print(X_transformed_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.impute import KNNImputer, SimpleImputer\n",
    "#from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = [\n",
    "    'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "# Create the ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', \n",
    "         Pipeline(steps=[\n",
    "             ('knn_imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "             ('scaler', RobustScaler())\n",
    "         ]), \n",
    "         numerical_cols),\n",
    "        \n",
    "        ('cat', \n",
    "         Pipeline(steps=[\n",
    "             ('simple_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "         ]), \n",
    "         categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model pipeline to the training data\n",
    "# Split the data into training and testing sets\n",
    "X = adult_dt.drop(columns='income')\n",
    "Y = adult_dt['income']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# You can now use model_pipeline to make predictions on the test set\n",
    "Y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Optionally, print the predictions\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ryanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\ryanw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Negative Log Loss: 0.0803 Â± 0.0005\n",
      "ROC AUC: 1.0000 Â± 0.0000\n",
      "Accuracy: 1.0000 Â± 0.0000\n",
      "Balanced Accuracy: 1.0000 Â± 0.0000\n",
      "\n",
      "Validation Results:\n",
      "Negative Log Loss: 0.3649 Â± 0.0205\n",
      "ROC AUC: 0.7749 Â± 0.0052\n",
      "Accuracy: 0.8542 Â± 0.0034\n",
      "Balanced Accuracy: 0.7749 Â± 0.0052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = [\n",
    "    'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "# Create the ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', \n",
    "         Pipeline(steps=[\n",
    "             ('knn_imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "             ('scaler', RobustScaler())\n",
    "         ]), \n",
    "         numerical_cols),\n",
    "        \n",
    "        ('cat', \n",
    "         Pipeline(steps=[\n",
    "             ('simple_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "         ]), \n",
    "         categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = adult_dt.drop(columns='income')\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Evaluate the model pipeline using cross-validation\n",
    "scoring = {\n",
    "    'neg_log_loss': make_scorer(log_loss, needs_proba=True),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'balanced_accuracy': make_scorer(balanced_accuracy_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cv_results = cross_validate(model_pipeline, X, Y, cv=5, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# Report the training and validation results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Negative Log Loss: {cv_results['train_neg_log_loss'].mean():.4f} Â± {cv_results['train_neg_log_loss'].std():.4f}\")\n",
    "print(f\"ROC AUC: {cv_results['train_roc_auc'].mean():.4f} Â± {cv_results['train_roc_auc'].std():.4f}\")\n",
    "print(f\"Accuracy: {cv_results['train_accuracy'].mean():.4f} Â± {cv_results['train_accuracy'].std():.4f}\")\n",
    "print(f\"Balanced Accuracy: {cv_results['train_balanced_accuracy'].mean():.4f} Â± {cv_results['train_balanced_accuracy'].std():.4f}\")\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(f\"Negative Log Loss: {cv_results['test_neg_log_loss'].mean():.4f} Â± {cv_results['test_neg_log_loss'].std():.4f}\")\n",
    "print(f\"ROC AUC: {cv_results['test_roc_auc'].mean():.4f} Â± {cv_results['test_roc_auc'].std():.4f}\")\n",
    "print(f\"Accuracy: {cv_results['test_accuracy'].mean():.4f} Â± {cv_results['test_accuracy'].std():.4f}\")\n",
    "print(f\"Balanced Accuracy: {cv_results['test_balanced_accuracy'].mean():.4f} Â± {cv_results['test_balanced_accuracy'].std():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold  Negative Log Loss   ROC AUC  Accuracy  Balanced Accuracy\n",
      "2     3           0.348129  0.778899  0.852733           0.778899\n",
      "4     5           0.350558  0.780176  0.857647           0.780176\n",
      "3     4           0.358425  0.777983  0.857955           0.777983\n",
      "0     1           0.362914  0.770252  0.853984           0.770252\n",
      "1     2           0.404425  0.767124  0.848741           0.767124\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for fold-level results\n",
    "fold_results = pd.DataFrame({\n",
    "    'Fold': range(1, 6),\n",
    "    'Negative Log Loss': cv_results['test_neg_log_loss'],\n",
    "    'ROC AUC': cv_results['test_roc_auc'],\n",
    "    'Accuracy': cv_results['test_accuracy'],\n",
    "    'Balanced Accuracy': cv_results['test_balanced_accuracy']\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by Negative Log Loss\n",
    "fold_results_sorted = fold_results.sort_values(by='Negative Log Loss')\n",
    "\n",
    "# Display the sorted fold-level results\n",
    "print(fold_results_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of Each Metric:\n",
      "Negative Log Loss    0.364890\n",
      "ROC AUC              0.774887\n",
      "Accuracy             0.854212\n",
      "Balanced Accuracy    0.774887\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean of each metric\n",
    "mean_results = fold_results_sorted[['Negative Log Loss', 'ROC AUC', 'Accuracy', 'Balanced Accuracy']].mean()\n",
    "\n",
    "# Display the sorted fold-level results and mean metrics\n",
    "#print(fold_results_sorted)\n",
    "print(\"\\nMean of Each Metric:\")\n",
    "print(mean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics on Testing Data:\n",
      "{'Negative Log Loss': 0.38733674303408167, 'ROC AUC': np.float64(0.9018825137514803), 'Accuracy': 0.8564847988535162, 'Balanced Accuracy': np.float64(0.7784191753807199)}\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = [\n",
    "    'age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'native-country'\n",
    "]\n",
    "\n",
    "# Create the ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', \n",
    "         Pipeline(steps=[\n",
    "             ('knn_imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "             ('scaler', RobustScaler())\n",
    "         ]), \n",
    "         numerical_cols),\n",
    "        \n",
    "        ('cat', \n",
    "         Pipeline(steps=[\n",
    "             ('simple_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "             ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "         ]), \n",
    "         categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = adult_dt.drop(columns='income')\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit the model pipeline to the training data\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions and get predicted probabilities\n",
    "Y_pred = model_pipeline.predict(X_test)\n",
    "Y_pred_proba = model_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "performance_metrics = {\n",
    "    'Negative Log Loss': log_loss(Y_test, Y_pred_proba),\n",
    "    'ROC AUC': roc_auc_score(Y_test, Y_pred_proba[:, 1]),\n",
    "    'Accuracy': accuracy_score(Y_test, Y_pred),\n",
    "    'Balanced Accuracy': balanced_accuracy_score(Y_test, Y_pred)\n",
    "}\n",
    "\n",
    "# Display the results as a dictionary\n",
    "print(\"Performance Metrics on Testing Data:\")\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Answer here.)\n",
    "\n",
    "[Answer] Recoding the target variable upon loading the data is a best practice that streamlines the data preprocessing pipeline, making it easier to work with the data and ensuring compatibility with modeling tools and functions. It enhances both the efficiency of the workflow and the clarity of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
